{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb0542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import category_encoders as ce\n",
    "import xfeat\n",
    "import texthero as hero\n",
    "from lightgbm import LGBMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59040fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "INPUT = \"../input\"\n",
    "SUBMISSION = \"../submission\"\n",
    "NAME = \"baseline001\"\n",
    "FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d78879",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorizontalDisplay:\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        template = '<div style=\"float: left; padding: 10px;\">{0}</div>'\n",
    "        return \"\\n\".join(template.format(arg._repr_html_())\n",
    "                         for arg in self.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b39050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def goal2feature(input_df):\n",
    "    tmp = input_df[\"goal\"]\n",
    "    tmp = tmp.replace(\"100000+\", \"100000-100000\")\n",
    "    tmp = np.array([g.split(\"-\") for g in tmp], dtype=\"int\")\n",
    "    output_df = pd.DataFrame(tmp, columns=[\"goal_min\", \"goal_max\"])\n",
    "    output_df[\"goal_upper_flag\"] = output_df[\"goal_min\"] == 100000\n",
    "    output_df[\"goal_lower_flag\"] = output_df[\"goal_min\"] == 1\n",
    "    output_df[\"goal_mean\"] = output_df[[\"goal_min\", \"goal_max\"]].mean(axis=1)\n",
    "    output_df[\"goal_q25\"] = output_df[[\"goal_min\", \"goal_max\"]].quantile(q=0.25, axis=1)\n",
    "    output_df[\"goal_q75\"] = output_df[[\"goal_min\", \"goal_max\"]].quantile(q=0.75, axis=1)\n",
    "    return output_df\n",
    "\n",
    "def get_numerical_feature(input_df):\n",
    "    cols = [\"duration\"]\n",
    "    return input_df[cols].copy()\n",
    "\n",
    "## binning\n",
    "def get_bins(input_df):\n",
    "    _input_df = pd.concat([\n",
    "        input_df[[\"duration\"]],\n",
    "        goal2feature(input_df),\n",
    "    ], axis=1)\n",
    "    output_df = pd.DataFrame()\n",
    "    output_df[\"bins_duration\"] = pd.cut(_input_df[\"duration\"],\n",
    "                                        bins=[-1, 30, 45, 60, 100],\n",
    "                                        labels=['bins_d1', 'bins_d2', 'bins_d3', 'bins_d4'])\n",
    "    output_df[\"bins_goal\"] = pd.cut(_input_df[\"goal_max\"],\n",
    "                                    bins=[-1, 19999, 49999, 79999, 99999, np.inf],\n",
    "                                    labels=['bins_g1', 'bins_g2', 'bins_g3', 'bins_g4', 'bins_g5'])\n",
    "    return output_df.astype(str)\n",
    "\n",
    "def show_scatterplot(input_df, x, y, hue=None, reg=True, title=None):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if hue is not None:\n",
    "        input_df = input_df.sort_values(hue)\n",
    "    if reg:\n",
    "        sns.regplot(x=x, y=y, data=input_df, scatter=False,color=\"red\", )\n",
    "    sns.scatterplot(data=input_df,x=x, y=y, hue=hue, s=200, palette='Set1', alpha=0.5)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b76e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_cat_features(input_df):\n",
    "    _input_df = pd.concat([\n",
    "        input_df,\n",
    "        get_bins(input_df)\n",
    "    ], axis=1).astype(str)\n",
    "    output_df = pd.DataFrame()\n",
    "    output_df[\"category3\"] = _input_df[\"category1\"] + _input_df[\"category2\"] \n",
    "    output_df[\"country+category1\"] = _input_df[\"country\"] + _input_df[\"category1\"]\n",
    "    output_df[\"country+category2\"] = _input_df[\"country\"] + _input_df[\"category2\"]\n",
    "    output_df[\"country+category3\"] = _input_df[\"country\"] + output_df[\"category3\"]\n",
    "    output_df[\"bins_DurationGoal\"] = _input_df[\"bins_duration\"] + _input_df[\"bins_goal\"]\n",
    "    return output_df\n",
    "\n",
    "def get_cross_num_features(input_df):\n",
    "    _input_df = pd.concat([\n",
    "        input_df,\n",
    "        goal2feature(input_df), \n",
    "    ], axis=1)\n",
    "    output_df = pd.DataFrame()\n",
    "    output_df[\"ratio_goalMax_duration\"] = _input_df[\"goal_max\"] / (_input_df[\"duration\"] + 1)\n",
    "    output_df[\"ratio_goalMin_duration\"] = _input_df[\"goal_min\"] / (_input_df[\"duration\"] + 1)\n",
    "    output_df[\"ratio_goalMean_duration\"] = _input_df[\"goal_mean\"] / (_input_df[\"duration\"] + 1)\n",
    "    output_df[\"prod_goalMax_duration\"] = _input_df[\"goal_max\"] * (_input_df[\"duration\"])\n",
    "    output_df[\"prod_goalMin_duration\"] = _input_df[\"goal_min\"] * (_input_df[\"duration\"])\n",
    "    output_df[\"prod_goalMean_duration\"] = _input_df[\"goal_mean\"] * (_input_df[\"duration\"])\n",
    "    return output_df\n",
    "\n",
    "def get_ce_features(input_df):\n",
    "    _input_df = pd.concat([\n",
    "        input_df, \n",
    "        get_cross_cat_features(input_df),\n",
    "        get_bins(input_df)\n",
    "    ], axis=1).astype(str)\n",
    "    cols = [\n",
    "        \"category1\",\n",
    "        \"category2\",\n",
    "        \"category3\",\n",
    "        \"country\",\n",
    "        \"country+category1\",\n",
    "        \"country+category2\",\n",
    "        \"country+category3\",\n",
    "        \"bins_duration\",\n",
    "        \"bins_goal\",\n",
    "        \"bins_DurationGoal\",\n",
    "    ]\n",
    "    encoder = ce.CountEncoder()\n",
    "    output_df = encoder.fit_transform(_input_df[cols]).add_prefix(\"CE_\")\n",
    "    return output_df\n",
    "\n",
    "def agg_country(input_df):\n",
    "    _input_df = pd.concat([\n",
    "        input_df,\n",
    "        goal2feature(input_df),\n",
    "        get_cross_num_features(input_df),\n",
    "        get_cross_cat_features(input_df),\n",
    "    ], axis=1)\n",
    "    group_key = \"country\"  # カテゴリ変数\n",
    "    group_values = [  # 集約される数値特徴量\n",
    "        \"goal_min\",\n",
    "        \"goal_max\",\n",
    "        \"goal_mean\",\n",
    "        \"duration\",\n",
    "        \"ratio_goalMax_duration\",\n",
    "        \"ratio_goalMin_duration\",\n",
    "        \"prod_goalMax_duration\",\n",
    "        \"prod_goalMin_duration\",\n",
    "    ]\n",
    "    agg_methods = [\"min\", \"max\", \"mean\", \"std\", \"count\"]  # 集約方法\n",
    "    output_df, cols = xfeat.aggregation(_input_df, group_key, group_values, agg_methods)\n",
    "    return output_df[cols].copy()\n",
    "\n",
    "def agg_category1(input_df):\n",
    "    _input_df = pd.concat([\n",
    "        input_df,\n",
    "        goal2feature(input_df),\n",
    "        get_cross_num_features(input_df),\n",
    "        get_cross_cat_features(input_df),\n",
    "    ], axis=1)\n",
    "    group_key = \"category1\"\n",
    "    group_values = [\n",
    "        \"goal_min\",\n",
    "        \"goal_max\",\n",
    "        \"goal_mean\",\n",
    "        \"duration\",\n",
    "        \"ratio_goalMax_duration\",\n",
    "        \"ratio_goalMin_duration\",\n",
    "        \"prod_goalMax_duration\",\n",
    "        \"prod_goalMin_duration\",\n",
    "    ]\n",
    "    agg_methods = [\"min\", \"max\", \"mean\", \"std\", \"count\"]\n",
    "    output_df, cols = xfeat.aggregation(_input_df, group_key, group_values, agg_methods)\n",
    "    return output_df[cols].copy()\n",
    "\n",
    "def agg_category2(input_df):\n",
    "    _input_df = pd.concat([\n",
    "        input_df,\n",
    "        goal2feature(input_df),\n",
    "        get_cross_num_features(input_df),\n",
    "        get_cross_cat_features(input_df),\n",
    "    ], axis=1)\n",
    "    group_key = \"category2\"\n",
    "    group_values = [\n",
    "        \"goal_min\",\n",
    "        \"goal_max\",\n",
    "        \"goal_mean\",\n",
    "        \"duration\",\n",
    "        \"ratio_goalMax_duration\",\n",
    "        \"ratio_goalMin_duration\",\n",
    "        \"prod_goalMax_duration\",\n",
    "        \"prod_goalMin_duration\",\n",
    "    ]\n",
    "    agg_methods = [\"min\", \"max\", \"mean\", \"std\", \"count\"]\n",
    "    output_df, cols = xfeat.aggregation(_input_df, group_key, group_values, agg_methods)\n",
    "    return output_df[cols].copy()\n",
    "\n",
    "\n",
    "def agg_category3(input_df):\n",
    "    _input_df = pd.concat([\n",
    "        input_df,\n",
    "        goal2feature(input_df),\n",
    "        get_cross_num_features(input_df),\n",
    "        get_cross_cat_features(input_df),\n",
    "    ], axis=1)\n",
    "    group_key = \"category3\"\n",
    "    group_values = [\n",
    "        \"goal_min\",\n",
    "        \"goal_max\",\n",
    "        \"goal_mean\",\n",
    "        \"duration\",\n",
    "        \"ratio_goalMax_duration\",\n",
    "        \"ratio_goalMin_duration\",\n",
    "        \"prod_goalMax_duration\",\n",
    "        \"prod_goalMin_duration\",\n",
    "    ]\n",
    "    agg_methods = [\"min\", \"max\", \"mean\", \"std\", \"count\"]\n",
    "    output_df, cols = xfeat.aggregation(_input_df, group_key, group_values, agg_methods)\n",
    "    return output_df[cols].copy()\n",
    "\n",
    "def agg_bins_duration_goal(input_df):\n",
    "    _input_df = pd.concat([\n",
    "        input_df,\n",
    "        goal2feature(input_df),\n",
    "        get_cross_num_features(input_df),\n",
    "        get_cross_cat_features(input_df),\n",
    "    ], axis=1)\n",
    "    group_key = \"bins_DurationGoal\"\n",
    "    group_values = [\n",
    "        \"goal_min\",\n",
    "        \"goal_max\",\n",
    "        \"goal_mean\",\n",
    "        \"duration\",\n",
    "        \"ratio_goalMax_duration\",\n",
    "        \"ratio_goalMin_duration\",\n",
    "        \"prod_goalMax_duration\",\n",
    "        \"prod_goalMin_duration\",\n",
    "    ]\n",
    "    agg_methods = [\"min\", \"max\", \"mean\", \"std\", \"count\"]\n",
    "    output_df, cols = xfeat.aggregation(_input_df, group_key, group_values, agg_methods)\n",
    "    return output_df[cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59796706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing_hero_remove_html_tags(input_df, text_col):\n",
    "    ## only remove html tags, do not remove punctuation\n",
    "    custom_pipeline = [\n",
    "        hero.preprocessing.fillna,\n",
    "        hero.preprocessing.remove_html_tags,\n",
    "        hero.preprocessing.lowercase,\n",
    "        hero.preprocessing.remove_digits,\n",
    "        hero.preprocessing.remove_stopwords,\n",
    "        hero.preprocessing.remove_whitespace,\n",
    "        hero.preprocessing.stem\n",
    "    ]\n",
    "    texts = hero.clean(input_df[text_col], custom_pipeline)\n",
    "    return texts\n",
    "\n",
    "\n",
    "def cleansing_hero_only_text(input_df, text_col):\n",
    "    ## get only text (remove html tags, punctuation & digits)\n",
    "    custom_pipeline = [\n",
    "        hero.preprocessing.fillna,\n",
    "        hero.preprocessing.remove_html_tags,\n",
    "        hero.preprocessing.lowercase,\n",
    "        hero.preprocessing.remove_digits,\n",
    "        hero.preprocessing.remove_punctuation,\n",
    "        hero.preprocessing.remove_diacritics,\n",
    "        hero.preprocessing.remove_stopwords,\n",
    "        hero.preprocessing.remove_whitespace,\n",
    "        hero.preprocessing.stem\n",
    "    ]\n",
    "    texts = hero.clean(input_df[text_col], custom_pipeline)\n",
    "    return texts\n",
    "# ------------------------------------------------------------ #\n",
    "\n",
    "# text の基本的な情報をgetする関数\n",
    "def basic_text_features_transformer(input_df, text_columns, cleansing_hero=None, name=\"\"):\n",
    "    def _get_features(dataframe, column):\n",
    "        _df = pd.DataFrame()\n",
    "        _df[column + name + '_num_chars'] = dataframe[column].apply(len)\n",
    "        _df[column + name + '_num_exclamation_marks'] = dataframe[column].apply(lambda x: x.count('!'))\n",
    "        _df[column + name + '_num_question_marks'] = dataframe[column].apply(lambda x: x.count('?'))\n",
    "        _df[column + name + '_num_punctuation'] = dataframe[column].apply(lambda x: sum(x.count(w) for w in '.,;:'))\n",
    "        _df[column + name + '_num_symbols'] = dataframe[column].apply(lambda x: sum(x.count(w) for w in '*&$%'))\n",
    "        _df[column + name + '_num_words'] = dataframe[column].apply(lambda x: len(x.split()))\n",
    "        _df[column + name + '_num_unique_words'] = dataframe[column].apply(lambda x: len(set(w for w in x.split())))\n",
    "        _df[column + name + '_words_vs_unique'] = _df[column + name + '_num_unique_words'] / _df[column + name + '_num_words']\n",
    "        _df[column + name + '_words_vs_chars'] = _df[column + name + '_num_words'] / _df[column + name + '_num_chars']\n",
    "        return _df\n",
    "    \n",
    "    # main の処理\n",
    "    output_df = pd.DataFrame()\n",
    "    output_df[text_columns] = input_df[text_columns].astype(str).fillna('missing')\n",
    "    for c in text_columns:\n",
    "        if cleansing_hero is not None:\n",
    "            output_df[c] = cleansing_hero(output_df, c)\n",
    "        output_df = _get_features(output_df, c)\n",
    "    return output_df\n",
    "\n",
    "# カウントベースの text vector をgetする関数 \n",
    "def text_vectorizer(input_df, \n",
    "                    text_columns,\n",
    "                    cleansing_hero=None,\n",
    "                    vectorizer=CountVectorizer(),\n",
    "                    transformer=TruncatedSVD(n_components=128),\n",
    "                    name='html_count_svd'):\n",
    "    \n",
    "    output_df = pd.DataFrame()\n",
    "    output_df[text_columns] = input_df[text_columns].astype(str).fillna('missing')\n",
    "    features = []\n",
    "    for c in text_columns:\n",
    "        if cleansing_hero is not None:\n",
    "            output_df[c] = cleansing_hero(output_df, c)\n",
    "\n",
    "        sentence = vectorizer.fit_transform(output_df[c])\n",
    "        feature = transformer.fit_transform(sentence)\n",
    "        num_p = feature.shape[1]\n",
    "        feature = pd.DataFrame(feature, columns=[name+str(num_p) + f'_{i:03}' for i in range(num_p)])\n",
    "        features.append(feature)\n",
    "    output_df = pd.concat(features, axis=1)\n",
    "    return output_df\n",
    "\n",
    "\n",
    "\n",
    "def get_text_vector_only_text__tfidf_sdv64(input_df):\n",
    "    output_df = text_vectorizer(input_df,\n",
    "                                [\"html_content\"],\n",
    "                                vectorizer=TfidfVectorizer(),\n",
    "                                cleansing_hero=cleansing_hero_only_text,  # hero\n",
    "                                transformer=TruncatedSVD(n_components=64, random_state=2021),\n",
    "                                name=\"only_text_html_tfidf_sdv\"\n",
    "                                )\n",
    "    return output_df\n",
    "\n",
    "def get_process_funcs_without_text():\n",
    "    funcs = [\n",
    "        goal2feature,\n",
    "        get_numerical_feature,\n",
    "        get_ce_features,\n",
    "        get_cross_num_features,\n",
    "        agg_country,\n",
    "        agg_category1,\n",
    "        agg_category2,\n",
    "        agg_category3,\n",
    "        agg_bins_duration_goal,\n",
    "    ]\n",
    "    return funcs\n",
    "\n",
    "# html_content 含めた特徴量作成\n",
    "def get_process_funcs():\n",
    "    funcs = [\n",
    "        goal2feature,\n",
    "        get_numerical_feature,\n",
    "        get_ce_features,\n",
    "        get_cross_num_features,\n",
    "        agg_country,\n",
    "        agg_category1,\n",
    "        agg_category2,\n",
    "        agg_category3,\n",
    "        agg_bins_duration_goal,\n",
    "        get_basic_text_features_raw,\n",
    "        get_basic_text_features_removed_html_tags,\n",
    "        get_text_vector_raw__tfidf_sdv64,\n",
    "        get_text_vector_removed_htnl_tags__tfidf_sdv64,\n",
    "        get_text_vector_only_text__tfidf_sdv64,\n",
    "    ]\n",
    "    return funcs\n",
    "\n",
    "def to_feature(input_df, funcs):\n",
    "    output_df = pd.DataFrame()\n",
    "    for func in tqdm(funcs, total=len(funcs)):\n",
    "        _df = func(input_df)\n",
    "        assert len(_df) == len(input_df), func.__name__\n",
    "        output_df = pd.concat([output_df, _df], axis=1)\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649a7a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_skf(train_x, train_y, random_state=2021):\n",
    "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=random_state)\n",
    "    folds_idx = [(t, v) for (t, v) in skf.split(train_x, train_y)]\n",
    "    return folds_idx\n",
    "\n",
    "def threshold_optimization(y_true, y_pred, metrics=None):\n",
    "    def f1_opt(x):\n",
    "        if metrics is not None:\n",
    "            score = -metrics(y_true, y_pred >= x)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return score\n",
    "    result = minimize(f1_opt, x0=np.array([0.5]), method='Nelder-Mead')\n",
    "    best_threshold = result['x'].item()\n",
    "    return best_threshold\n",
    "\n",
    "\n",
    "def optimized_f1(y_true, y_pred):\n",
    "    bt = threshold_optimization(y_true, y_pred, metrics=f1_score)\n",
    "    score = f1_score(y_true, y_pred >= bt)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb21590",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLGBMModel:\n",
    "    def __init__(self, name=None, params=None, fold=None, train_x=None, train_y=None, test_x=None, metrics=None, seeds=None):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.test_x = test_x\n",
    "        self.name = name\n",
    "        self.params = params\n",
    "        self.metrics = metrics  \n",
    "        self.kfold = fold  \n",
    "        self.oof = None\n",
    "        self.preds = None\n",
    "        self.seeds = seeds if seeds is not None else [2020]  \n",
    "        self.models = {}  \n",
    "\n",
    "    def build_model(self):\n",
    "        model = LGBMModel(**self.params)\n",
    "        return model\n",
    "\n",
    "    def predict_cv(self):\n",
    "        oof_seeds = []\n",
    "        scores_seeds = []\n",
    "        for seed in self.seeds:\n",
    "            oof = []\n",
    "            va_idxes = []\n",
    "            scores = []\n",
    "            train_x = self.train_x.values\n",
    "            train_y = self.train_y.values\n",
    "            fold_idx = self.kfold(self.train_x, self.train_y, random_state=seed) \n",
    "\n",
    "         \n",
    "            for cv_num, (tr_idx, va_idx) in enumerate(fold_idx):\n",
    "                tr_x, va_x = train_x[tr_idx], train_x[va_idx]\n",
    "                tr_y, va_y = train_y[tr_idx], train_y[va_idx]\n",
    "                va_idxes.append(va_idx)\n",
    "                model = self.build_model()\n",
    "    \n",
    "        \n",
    "                model.fit(tr_x, tr_y,\n",
    "                          eval_set=[[va_x, va_y]],\n",
    "                          early_stopping_rounds=100,\n",
    "                          verbose=False)  \n",
    "                model_name = f\"{self.name}_SEED{seed}_FOLD{cv_num}_model.pkl\"\n",
    "                self.models[model_name] = model  \n",
    "                \n",
    "\n",
    "                pred = model.predict(va_x)\n",
    "                oof.append(pred)\n",
    "\n",
    "                score = self.get_score(va_y, pred)\n",
    "                scores.append(score)\n",
    "                print(f\"SEED:{seed}, FOLD:{cv_num} =====> val_score:{score}\")\n",
    "\n",
    "\n",
    "            va_idxes = np.concatenate(va_idxes)\n",
    "            oof = np.concatenate(oof)\n",
    "            order = np.argsort(va_idxes)\n",
    "            oof = oof[order]\n",
    "            oof_seeds.append(oof)\n",
    "            scores_seeds.append(np.mean(scores))\n",
    "            \n",
    "        oof = np.mean(oof_seeds, axis=0)\n",
    "        self.oof = oof\n",
    "        print(f\"model:{self.name} score:{self.get_score(self.train_y, oof)}\\n\")\n",
    "        return oof\n",
    "\n",
    "    def inference(self):\n",
    "        preds_seeds = []\n",
    "        for seed in self.seeds:\n",
    "            preds = []\n",
    "            test_x = self.test_x.values\n",
    "            for cv_num in range(FOLDS):\n",
    "                print(f\"-INFERENCE- SEED:{seed}, FOLD:{cv_num}\")\n",
    "\n",
    "                model_name = f\"{self.name}_SEED{seed}_FOLD{cv_num}_model.pkl\"\n",
    "                model = self.models[model_name]\n",
    "                pred = model.predict(test_x)\n",
    "                preds.append(pred)\n",
    "            preds = np.mean(preds, axis=0)\n",
    "            preds_seeds.append(preds)\n",
    "        preds = np.mean(preds_seeds, axis=0)\n",
    "        self.preds = preds\n",
    "        return preds\n",
    "\n",
    "    def tree_importance(self):\n",
    "        # visualize feature importance\n",
    "        feature_importance_df = pd.DataFrame()\n",
    "        for i, (tr_idx, va_idx) in enumerate(self.kfold(self.train_x, self.train_y)):\n",
    "            tr_x, va_x = self.train_x.values[tr_idx], self.train_x.values[va_idx]\n",
    "            tr_y, va_y = self.train_y.values[tr_idx], self.train_y.values[va_idx]\n",
    "            model = self.build_model()\n",
    "            model.fit(tr_x, tr_y,\n",
    "                      eval_set=[[va_x, va_y]],\n",
    "                      early_stopping_rounds=100,\n",
    "                      verbose=False) \n",
    "            _df = pd.DataFrame()\n",
    "            _df['feature_importance'] = model.feature_importances_\n",
    "            _df['column'] = self.train_x.columns\n",
    "            _df['fold'] = i + 1\n",
    "            feature_importance_df = pd.concat([feature_importance_df, _df], axis=0, ignore_index=True)\n",
    "        order = feature_importance_df.groupby('column') \\\n",
    "                    .sum()[['feature_importance']] \\\n",
    "                    .sort_values('feature_importance', ascending=False).index[:50]\n",
    "        fig, ax = plt.subplots(figsize=(12, max(4, len(order) * .2)))\n",
    "        sns.boxenplot(data=feature_importance_df, y='column', x='feature_importance', order=order, ax=ax,\n",
    "                      palette='viridis')\n",
    "        fig.tight_layout()\n",
    "        ax.grid()\n",
    "        ax.set_title('feature importance')\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "        return fig, feature_importance_df\n",
    "    \n",
    "    def get_score(self, y_true, y_pred):\n",
    "        score = self.metrics(y_true, y_pred)\n",
    "        return score\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60a4e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_optimization(y_true, y_pred, metrics=None):\n",
    "    def f1_opt(x):\n",
    "        if metrics is not None:\n",
    "            score = -metrics(y_true, y_pred >= x)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return score\n",
    "    result = minimize(f1_opt, x0=np.array([0.5]), method='Nelder-Mead')\n",
    "    best_threshold = result['x'].item()\n",
    "    return best_threshold\n",
    "\n",
    "\n",
    "def optimized_f1(y_true, y_pred):\n",
    "    bt = threshold_optimization(y_true, y_pred, metrics=f1_score)\n",
    "    score = f1_score(y_true, y_pred >= bt)\n",
    "    return score\n",
    "def visualize_confusion_matrix(y_true,\n",
    "                               pred_label,\n",
    "                               height=.6,\n",
    "                               labels=None):\n",
    "    conf = confusion_matrix(y_true=y_true,\n",
    "                            y_pred=pred_label,\n",
    "                            normalize='true')\n",
    "    n_labels = len(conf)\n",
    "    size = n_labels * height\n",
    "    fig, ax = plt.subplots(figsize=(size * 4, size * 3))\n",
    "    sns.heatmap(conf, cmap='Blues', ax=ax, annot=True, fmt='.2f')\n",
    "    ax.set_ylabel('Label')\n",
    "    ax.set_xlabel('Predict')\n",
    "\n",
    "    if labels is not None:\n",
    "        ax.set_yticklabels(labels)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.tick_params('y', labelrotation=0)\n",
    "        ax.tick_params('x', labelrotation=90)\n",
    "\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df26f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(INPUT, \"train.csv\"))\n",
    "test = pd.read_csv(os.path.join(INPUT, \"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971edff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.concat([train, test]).reset_index(drop=True) \n",
    "\n",
    "process_funcs = get_process_funcs()\n",
    "output_df = to_feature(input_df, process_funcs)\n",
    "train_x = output_df.iloc[:len(train)]\n",
    "test_x = output_df.iloc[len(train):].reset_index(drop=True)\n",
    "\n",
    "# target variable\n",
    "train_y = train[\"state\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f63d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"n_estimators\": 10000,\n",
    "    \"objective\": 'binary',\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"num_leaves\": 31,\n",
    "    \"random_state\": 811,\n",
    "    \"n_jobs\": -1,\n",
    "    \"importance_type\": \"gain\",\n",
    "    'colsample_bytree': .5,\n",
    "    \"reg_lambda\": 5,\n",
    "}\n",
    "model = MyLGBMModel(name=NAME, \n",
    "                    params=model_params,\n",
    "                    fold=make_skf,\n",
    "                    train_x=train_x,\n",
    "                    train_y=train_y,\n",
    "                    test_x=test_x,\n",
    "                    metrics=optimized_f1, \n",
    "                    seeds=[0, 1, 2]\n",
    "                   )\n",
    "\n",
    "\n",
    "fig, importance_df = model.tree_importance()\n",
    "\n",
    "\n",
    "selected_num = 30\n",
    "cols = importance_df.groupby(\"column\").mean().reset_index().sort_values(\"feature_importance\", ascending=False)[\"column\"].tolist()\n",
    "selected_cols = cols[:selected_num]\n",
    "model.train_x = model.train_x[selected_cols]\n",
    "model.test_x = model.test_x[selected_cols]\n",
    "\n",
    "\n",
    "oof = model.predict_cv()  \n",
    "preds = model.inference()  \n",
    "\n",
    "best_threshold = threshold_optimization(y_true=train_y, y_pred=oof, metrics=f1_score) \n",
    "print(f\"best_threshold is {best_threshold}\\n\")\n",
    "\n",
    "\n",
    "visualize_confusion_matrix(y_true=train_y,\n",
    "                           pred_label=oof>=best_threshold)\n",
    "\n",
    "\n",
    "labels = preds >= best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9b3ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv(os.path.join(INPUT, \"sample_submit.csv\"), header=None)\n",
    "sub_notext = sample_sub.copy()\n",
    "sub = sample_sub.copy()\n",
    "\n",
    "\n",
    "sub[1] = labels\n",
    "sub = sub.astype(int)\n",
    "sub.to_csv(os.path.join(SUBMISSION, f'{NAME}.csv'), index=False, header=False)\n",
    "\n",
    "sub_notext.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ae5092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d2a70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb7b60d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47a0234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9425a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872348b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dafd74c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed00c651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4471c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9a7bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ff2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd39ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
